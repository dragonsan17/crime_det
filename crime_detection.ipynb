{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crime_detection",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9FGh6+HZAHaqyCAa32LeS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dragonsan17/crime_det/blob/master/crime_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zABc2aTgKT11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import nltk \n",
        "import string\n",
        "import re\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BLmuvIRLZ9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOa3jXH5Lx_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile \n",
        "file_name = \"/content/gdrive/My Drive/filtered_tweets_JK.json.zip\"\n",
        "  \n",
        "# opening the zip file in READ mode \n",
        "with ZipFile(file_name, 'r') as zip: \n",
        "    zip.printdir() \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall() \n",
        "    print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ-9PE4PNyWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eng_tweets = []\n",
        "for tweet in tweets:\n",
        "  if(tweet['lang']==\"en\"):\n",
        "    eng_tweets.append(tweet['text'])\n",
        "print(\"appended \",len(eng_tweets),\" tweets\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yGJ4Izlb1yG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Block : clean text, return tokenized text\n",
        "def remove_punct(text):\n",
        "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    text = re.sub('[0-9]+', '', text)\n",
        "    return text\n",
        "ps = nltk.PorterStemmer()\n",
        "def clean_token(text):\n",
        "    #tokenize\n",
        "    text = re.split('\\W+', text)\n",
        "    #remove stop words\n",
        "    text = [word for word in text if word not in stopword]\n",
        "    text = [word for word in text if word is not '']\n",
        "    #stemming\n",
        "    text = [ps.stem(word) for word in text]\n",
        "    return text\n",
        "\n",
        "def clean_text(eng_tweets):\n",
        "  tokens = []\n",
        "  for i in range(len(eng_tweets)):\n",
        "      #remove handles, urls, RTs, emojis\n",
        "      eng_tweets[i] = \" \".join([word for word in eng_tweets[i].split()\n",
        "                                  if 'http' not in word and '@' not in word and '<' not in word])\n",
        "      eng_tweets[i] = eng_tweets[i].encode('ascii', 'ignore').decode('ascii')\n",
        "      #remove punctuations\n",
        "      for punct in '.,\"':\n",
        "        eng_tweets[i] = eng_tweets[i].replace(punct, ' ')\n",
        "      eng_tweets[i] = remove_punct(eng_tweets[i])\n",
        "      #clean token\n",
        "      tokens.append(clean_token(eng_tweets[i]))\n",
        "  return tokens\n",
        "\n",
        "tokens = clean_text(eng_tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wATCOsDiR3Rn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7e197c8-ae31-42c2-ea33-26e09f1cbfcd"
      },
      "source": [
        "crimes = ['hostage','murder','robbery','hostage','con','fraud','kidnap','rape','terrorism','terror','crime','protest','pelt','arson']\n",
        "cnt=0\n",
        "for token in tokens:\n",
        "  if any(word in crimes for word in token):\n",
        "    cnt+=1\n",
        "print(cnt)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "963\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}