{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crime_detection",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVaFSL0i97VwJHKeuW/TXD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dragonsan17/crime_det/blob/master/crime_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zABc2aTgKT11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a477fcc5-4df4-4764-8ff5-73db9b610430"
      },
      "source": [
        "import json\n",
        "import nltk \n",
        "import string\n",
        "import re\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BLmuvIRLZ9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "f874023f-0cef-4c50-a080-26d8cfc05f02"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOa3jXH5Lx_9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "a4d89bd5-1203-44e2-d98e-d486b9efee3a"
      },
      "source": [
        "from zipfile import ZipFile \n",
        "file_name = \"/content/gdrive/My Drive/filtered_tweets_JK.json.zip\"\n",
        "  \n",
        "# opening the zip file in READ mode \n",
        "with ZipFile(file_name, 'r') as zip: \n",
        "    zip.printdir() \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall() \n",
        "    print('Done!')\n",
        "data_path = '/content/filtered_tweets_JK.json'\n",
        "f = open(data_path,'r')\n",
        "tweets = json.load(f)\n",
        "print(type(tweets))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name                                             Modified             Size\n",
            "filtered_tweets_JK.json                        2019-07-23 11:44:12   1197687280\n",
            "Extracting all the files now...\n",
            "Done!\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ-9PE4PNyWR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "565e8322-4bb3-4a61-acd9-822a834b7790"
      },
      "source": [
        "eng_tweets = []\n",
        "for tweet in tweets:\n",
        "  if(tweet['lang']==\"en\"):\n",
        "    eng_tweets.append(tweet['text'])\n",
        "print(\"appended \",len(eng_tweets),\" tweets\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "appended  101743  tweets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yGJ4Izlb1yG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Block : clean text, return tokenized text\n",
        "def remove_punct(text):\n",
        "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    text = re.sub('[0-9]+', '', text)\n",
        "    return text\n",
        "ps = nltk.PorterStemmer()\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "def clean_token(text):\n",
        "    #tokenize\n",
        "    text = re.split('\\W+', text)\n",
        "    #remove stop words\n",
        "    text = [word for word in text if word not in stopword]\n",
        "    text = [word for word in text if word is not '']\n",
        "    #stemming\n",
        "    text = [ps.stem(word) for word in text]\n",
        "    return text\n",
        "\n",
        "def clean_text(eng_tweets):\n",
        "  tokens = []\n",
        "  for i in range(len(eng_tweets)):\n",
        "      #remove handles, urls, RTs, emojis, hashtags\n",
        "      original = eng_tweets[i]\n",
        "      eng_tweets[i] = \" \".join([word for word in eng_tweets[i].split()\n",
        "                                  if 'http' not in word and '@' not in word and '<' not in word and '#' not in word])\n",
        "      eng_tweets[i] = eng_tweets[i].encode('ascii', 'ignore').decode('ascii')\n",
        "      #remove punctuations\n",
        "      for punct in '.,\"':\n",
        "        eng_tweets[i] = eng_tweets[i].replace(punct, ' ')\n",
        "      eng_tweets[i] = remove_punct(eng_tweets[i])\n",
        "      #clean token\n",
        "      tokens.append((clean_token(eng_tweets[i]),original))\n",
        "  return tokens\n",
        "\n",
        "tokens = clean_text(eng_tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8w7esrql2Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crimes = ['hostage','murder','robbery','con','fraud','kidnap','rape','terrorism','terror','crime','protest','pelt','arson']\n",
        "crime_dict = {}\n",
        "for crime in crimes:\n",
        "  crime_dict[(crime,1)] = []\n",
        "  crime_dict[(crime,-1)] = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wATCOsDiR3Rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input +1 or -1 for each tweet\n",
        "# cnt=0\n",
        "for token,tweet in tokens[:50000]:\n",
        "  # if any(word in crimes for word in token):\n",
        "  #   cnt+=1\n",
        "  for word in token:\n",
        "    if word in crimes:\n",
        "      print(\"Crime : \",word,\" = \\n\",tweet)\n",
        "      sign_of_crime = int(input())\n",
        "      crime_dict[(word,sign_of_crime)].append((token,tweet))\n",
        "# print(cnt)\n",
        "# print(crime_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arNI42v_qjqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#saves crime keyword, nature of crime (positive or negative), tokens, and original tweet text in csv\n",
        "\n",
        "import csv\n",
        "with open('/content/gdrive/My Drive/final_tweet_data.csv', 'w', newline=\"\") as csv_file:  \n",
        "    writer = csv.writer(csv_file)\n",
        "    for (crime,sign), values in crime_dict.items():\n",
        "      for tokens,original_tweet in values:\n",
        "       writer.writerow([crime,sign,tokens,original_tweet])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}